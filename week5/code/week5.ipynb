{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b189babc",
   "metadata": {},
   "source": [
    "# Week 5: Pharmacogenomics Analysis\n",
    "\n",
    "## Analysis Pipeline\n",
    "1. Download reference genome (chromosome 10)\n",
    "2. Align Illumina and PacBio reads with minimap2\n",
    "3. Call variants with bcftools\n",
    "4. Phase variants with HapCUT2\n",
    "5. Compare VCFs and identify discordant variants\n",
    "6. Determine star-alleles using PharmVar database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a3341-7120-41ad-a56d-59b4e180b1b3",
   "metadata": {},
   "source": [
    "## Step 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e7094",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f92a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import urllib.request\n",
    "import shutil\n",
    "import bz2\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e513b54",
   "metadata": {},
   "source": [
    "### Set up working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c05f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code\n",
      "Data directory: /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data\n"
     ]
    }
   ],
   "source": [
    "notebook_dir = Path.cwd()\n",
    "data_dir = notebook_dir / \"data\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Working directory: {notebook_dir}\")\n",
    "print(f\"Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa2819",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "\n",
    "All target genes are located on chromosome 10:\n",
    "- CYP2C8\n",
    "- CYP2C9\n",
    "- CYP2C19\n",
    "\n",
    "Locate these genes in the reference genome. Use Genome Browser. We will focus on the hg38 (or GRCh38) version of the human genome. Also, make sure to download the reference genome!\n",
    "\n",
    "Note: you will need to download the human genome for this step; however, note that you do not need the whole human genome. Just focus on the chromosome that contains those genes! The reference should basically be a single FASTA file (extension: .fa or .fasta).\n",
    "\n",
    "Expected output: a single FASTA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a145cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOADING REFERENCE GENOME\n",
      "Downloading chromosome 10 from UCSC Genome Browser\n",
      "Source: https://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr10.fa.gz\n",
      "Target: /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/chr10.fa.gz\n",
      "Download complete!\n",
      "Decompressing chr10.fa.gz...\n",
      "Decompression complete!\n",
      "Output file: /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/chr10.fa\n",
      "Removed compressed file: /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/chr10.fa.gz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download chromosome 10 reference genome\n",
    "chr10_url = \"https://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr10.fa.gz\"\n",
    "chr10_gz_path = data_dir / \"chr10.fa.gz\"\n",
    "chr10_fa_path = data_dir / \"chr10.fa\"\n",
    "\n",
    "print(\"DOWNLOADING REFERENCE GENOME\")\n",
    "\n",
    "if not chr10_fa_path.exists():\n",
    "    print(f\"Downloading chromosome 10 from UCSC Genome Browser\")\n",
    "    print(f\"Source: {chr10_url}\")\n",
    "    print(f\"Target: {chr10_gz_path}\")\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlretrieve(chr10_url, chr10_gz_path)\n",
    "        print(\"Download complete!\")\n",
    "        \n",
    "        print(f\"Decompressing {chr10_gz_path.name}...\")\n",
    "        with gzip.open(chr10_gz_path, 'rb') as f_in:\n",
    "            with open(chr10_fa_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "        print(f\"Decompression complete!\")\n",
    "        print(f\"Output file: {chr10_fa_path}\")\n",
    "        \n",
    "        # Remove compressed file to save space\n",
    "        chr10_gz_path.unlink()\n",
    "        print(f\"Removed compressed file: {chr10_gz_path}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(f\"Reference genome already exists: {chr10_fa_path}\")\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download_data",
   "metadata": {},
   "source": [
    "### Download Sequencing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "download_sequencing_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Illumina data from GitHub\n",
      "Source: https://github.com/inumanag/fall25-csc-bioinf/raw/refs/heads/main/week4/data/illumina.fq.bz2\n",
      "Download complete!\n",
      "Decompressing illumina.fq.bz2...\n",
      "Decompression complete!\n",
      "Output file: /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/illumina.fq\n",
      "Removed compressed file: /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/illumina.fq.bz2\n",
      "\n",
      "Downloading PacBio data from GitHub\n",
      "Source: https://github.com/inumanag/fall25-csc-bioinf/raw/refs/heads/main/week4/data/pacbio.fq.bz2\n",
      "Download complete!\n",
      "Decompressing pacbio.fq.bz2...\n",
      "Decompression complete!\n",
      "Output file: /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/pacbio.fq\n",
      "Removed compressed file: /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/pacbio.fq.bz2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download Illumina data\n",
    "illumina_url = \"https://github.com/inumanag/fall25-csc-bioinf/raw/refs/heads/main/week4/data/illumina.fq.bz2\"\n",
    "illumina_bz2_path = data_dir / \"illumina.fq.bz2\"\n",
    "illumina_fq_path = data_dir / \"illumina.fq\"\n",
    "\n",
    "if not illumina_fq_path.exists():\n",
    "    print(f\"Downloading Illumina data from GitHub\")\n",
    "    print(f\"Source: {illumina_url}\")\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlretrieve(illumina_url, illumina_bz2_path)\n",
    "        print(\"Download complete!\")\n",
    "        \n",
    "        print(f\"Decompressing {illumina_bz2_path.name}...\")\n",
    "        with bz2.open(illumina_bz2_path, 'rb') as f_in:\n",
    "            with open(illumina_fq_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "        print(f\"Decompression complete!\")\n",
    "        print(f\"Output file: {illumina_fq_path}\")\n",
    "        \n",
    "        # Remove compressed file\n",
    "        illumina_bz2_path.unlink()\n",
    "        print(f\"Removed compressed file: {illumina_bz2_path}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(f\"Illumina data already exists: {illumina_fq_path}\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "pacbio_url = \"https://github.com/inumanag/fall25-csc-bioinf/raw/refs/heads/main/week4/data/pacbio.fq.bz2\"\n",
    "pacbio_bz2_path = data_dir / \"pacbio.fq.bz2\"\n",
    "pacbio_fq_path = data_dir / \"pacbio.fq\"\n",
    "\n",
    "if not pacbio_fq_path.exists():\n",
    "    print(f\"Downloading PacBio data from GitHub\")\n",
    "    print(f\"Source: {pacbio_url}\")\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlretrieve(pacbio_url, pacbio_bz2_path)\n",
    "        print(\"Download complete!\")\n",
    "        \n",
    "        print(f\"Decompressing {pacbio_bz2_path.name}...\")\n",
    "        with bz2.open(pacbio_bz2_path, 'rb') as f_in:\n",
    "            with open(pacbio_fq_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "        print(f\"Decompression complete!\")\n",
    "        print(f\"Output file: {pacbio_fq_path}\")\n",
    "        \n",
    "        pacbio_bz2_path.unlink()\n",
    "        print(f\"Removed compressed file: {pacbio_bz2_path}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(f\"PacBio data already exists: {pacbio_fq_path}\")\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa2819_align",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "\n",
    "Align all samples in FASTQ format to the human genome (version GRCh38). Use minimap2 for the alignment. Make sure to use appropriate parameters for each technology.\n",
    "\n",
    "Note: you will need to download the human genome for this step; however, note that you do not need the whole human genome. Just focus on the chromosome that contains those genes! The reference should basically be a single FASTA file (extension: .fa).\n",
    "\n",
    "Expected output: two BAM files and two BAI files (one of those for each sample)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7856f",
   "metadata": {},
   "source": [
    "### Index Reference Genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f992c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEXING REFERENCE GENOME\n",
      "Creating minimap2 index for /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/chr10.fa\n",
      "[M::mm_idx_gen::5.555*0.71] collected minimizers\n",
      "[M::mm_idx_gen::6.252*0.94] sorted minimizers\n",
      "[M::main::83.910*0.16] loaded/built the index for 1 target sequence(s)\n",
      "[M::mm_idx_stat] kmer size: 15; skip: 10; is_hpc: 0; #seq: 1\n",
      "[M::mm_idx_stat::84.082*0.16] distinct minimizers: 16061920 (79.91% are singletons); average occurrences: 1.563; average spacing: 5.329; total length: 133797422\n",
      "[M::main] Version: 2.26-r1175\n",
      "[M::main] CMD: minimap2 -d /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/chr10.mmi /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/chr10.fa\n",
      "[M::main] Real time: 84.148 sec; CPU: 13.487 sec; Peak RSS: 1.097 GB\n",
      "\n",
      "Index created: /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/chr10.mmi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"INDEXING REFERENCE GENOME\")\n",
    "\n",
    "chr10_mmi_path = data_dir / \"chr10.mmi\"\n",
    "\n",
    "if not chr10_mmi_path.exists():\n",
    "    print(f\"Creating minimap2 index for {chr10_fa_path}\")\n",
    "    cmd = [\n",
    "        \"minimap2\",\n",
    "        \"-d\", str(chr10_mmi_path),\n",
    "        str(chr10_fa_path)\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "    print(result.stderr)  # minimap2 outputs to stderr\n",
    "    print(f\"Index created: {chr10_mmi_path}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Index already exists: {chr10_mmi_path}\")\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8ba23",
   "metadata": {},
   "source": [
    "### Align Illumina Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064bb264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALIGNING ILLUMINA READS\n",
      "Aligning Illumina reads with minimap2 (sr preset for short reads)\n",
      "[WARNING]\u001b[1;31m Indexing parameters (-k, -w or -H) overridden by parameters used in the prebuilt index.\u001b[0m\n",
      "[M::main::73.124*0.12] loaded/built the index for 1 target sequence(s)\n",
      "[M::mm_mapopt_update::73.124*0.12] mid_occ = 1000\n",
      "[M::mm_idx_stat] kmer size: 15; skip: 10; is_hpc: 0; #seq: 1\n",
      "[M::mm_idx_stat::73.289*0.12] distinct minimizers: 16061920 (79.91% are singletons); average occurrences: 1.563; average spacing: 5.329; total length: 133797422\n",
      "[M::worker_pipeline::121.134*0.46] mapped 309505 sequences\n",
      "[M::main] Version: 2.26-r1175\n",
      "[M::main] CMD: minimap2 -ax sr -t 4 /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/chr10.mmi /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/illumina.fq\n",
      "[M::main] Real time: 121.157 sec; CPU: 55.691 sec; Peak RSS: 0.925 GB\n",
      "\n",
      "Alignment complete: /mnt/c/Users/Nick/Documents/GitHub/fall25-csc-bioinf/week5/code/data/illumina.sam\n",
      "Converting SAM to BAM...\n"
     ]
    }
   ],
   "source": [
    "print(\"ALIGNING ILLUMINA READS\")\n",
    "\n",
    "illumina_sam = data_dir / \"illumina.sam\"\n",
    "illumina_bam = data_dir / \"illumina.bam\"\n",
    "illumina_sorted_bam = data_dir / \"illumina_sorted.bam\"\n",
    "illumina_bai = data_dir / \"illumina_sorted.bam.bai\"\n",
    "\n",
    "if not illumina_sorted_bam.exists():\n",
    "    print(\"Aligning Illumina reads with minimap2 (sr preset for short reads)\")\n",
    "    \n",
    "    # Align with minimap2 using short-read preset\n",
    "    align_cmd = [\n",
    "        \"minimap2\",\n",
    "        \"-ax\", \"sr\",  # short single-end reads preset\n",
    "        \"-t\", \"4\",     # threads\n",
    "        str(chr10_mmi_path),\n",
    "        str(illumina_fq_path)\n",
    "    ]\n",
    "    \n",
    "    with open(illumina_sam, 'w') as sam_file:\n",
    "        result = subprocess.run(align_cmd, stdout=sam_file, stderr=subprocess.PIPE, text=True, check=True)\n",
    "        print(result.stderr)  # minimap2 outputs to stderr\n",
    "    \n",
    "    print(f\"Alignment complete: {illumina_sam}\")\n",
    "    \n",
    "    # Convert SAM to BAM\n",
    "    print(\"Converting SAM to BAM...\")\n",
    "    subprocess.run([\n",
    "        \"samtools\", \"view\",\n",
    "        \"-b\", \"-o\", str(illumina_bam),\n",
    "        str(illumina_sam)\n",
    "    ], check=True)\n",
    "    print(f\"BAM created: {illumina_bam}\")\n",
    "    \n",
    "    # Sort BAM\n",
    "    print(\"Sorting BAM file...\")\n",
    "    subprocess.run([\n",
    "        \"samtools\", \"sort\",\n",
    "        \"-o\", str(illumina_sorted_bam),\n",
    "        str(illumina_bam)\n",
    "    ], check=True)\n",
    "    print(f\"Sorted BAM created: {illumina_sorted_bam}\")\n",
    "    \n",
    "    # Index BAM\n",
    "    print(\"Indexing BAM file...\")\n",
    "    subprocess.run([\n",
    "        \"samtools\", \"index\",\n",
    "        str(illumina_sorted_bam)\n",
    "    ], check=True)\n",
    "    print(f\"Index created: {illumina_bai}\")\n",
    "    \n",
    "    # Clean up intermediate files\n",
    "    illumina_sam.unlink()\n",
    "    illumina_bam.unlink()\n",
    "    print(\"Cleaned up intermediate files\")\n",
    "    \n",
    "    print(f\"\\nFinal output: {illumina_sorted_bam}\")\n",
    "    print(f\"Final index: {illumina_bai}\")\n",
    "else:\n",
    "    print(f\"Illumina alignment already exists: {illumina_sorted_bam}\")\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf47b3",
   "metadata": {},
   "source": [
    "### Align PacBio Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ALIGNING PACBIO READS\")\n",
    "\n",
    "pacbio_sam = data_dir / \"pacbio.sam\"\n",
    "pacbio_bam = data_dir / \"pacbio.bam\"\n",
    "pacbio_sorted_bam = data_dir / \"pacbio_sorted.bam\"\n",
    "pacbio_bai = data_dir / \"pacbio_sorted.bam.bai\"\n",
    "\n",
    "if not pacbio_sorted_bam.exists():\n",
    "    print(\"Aligning PacBio reads with minimap2 (map-pb preset for PacBio)\")\n",
    "    \n",
    "    # Align with minimap2 using PacBio preset\n",
    "    align_cmd = [\n",
    "        \"minimap2\",\n",
    "        \"-ax\", \"map-pb\",  # PacBio CLR reads preset\n",
    "        \"-t\", \"4\",         # threads\n",
    "        str(chr10_mmi_path),\n",
    "        str(pacbio_fq_path)\n",
    "    ]\n",
    "    \n",
    "    with open(pacbio_sam, 'w') as sam_file:\n",
    "        result = subprocess.run(align_cmd, stdout=sam_file, stderr=subprocess.PIPE, text=True, check=True)\n",
    "        print(result.stderr)  # minimap2 outputs to stderr\n",
    "    \n",
    "    print(f\"Alignment complete: {pacbio_sam}\")\n",
    "    \n",
    "    print(\"Converting SAM to BAM...\")\n",
    "    subprocess.run([\n",
    "        \"samtools\", \"view\",\n",
    "        \"-b\", \"-o\", str(pacbio_bam),\n",
    "        str(pacbio_sam)\n",
    "    ], check=True)\n",
    "    print(f\"BAM created: {pacbio_bam}\")\n",
    "    \n",
    "    print(\"Sorting BAM file...\")\n",
    "    subprocess.run([\n",
    "        \"samtools\", \"sort\",\n",
    "        \"-o\", str(pacbio_sorted_bam),\n",
    "        str(pacbio_bam)\n",
    "    ], check=True)\n",
    "    print(f\"Sorted BAM created: {pacbio_sorted_bam}\")\n",
    "    \n",
    "    print(\"Indexing BAM file...\")\n",
    "    subprocess.run([\n",
    "        \"samtools\", \"index\",\n",
    "        str(pacbio_sorted_bam)\n",
    "    ], check=True)\n",
    "    print(f\"Index created: {pacbio_bai}\")\n",
    "    \n",
    "    pacbio_sam.unlink()\n",
    "    pacbio_bam.unlink()\n",
    "    print(\"Cleaned up intermediate files\")\n",
    "    \n",
    "    print(f\"\\nFinal output: {pacbio_sorted_bam}\")\n",
    "    print(f\"Final index: {pacbio_bai}\")\n",
    "else:\n",
    "    print(f\"PacBio alignment already exists: {pacbio_sorted_bam}\")\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd8635",
   "metadata": {},
   "source": [
    "## Step 3: Call Variants with bcftools\n",
    "\n",
    "Find all variants in each sample for all genes of interest and obtain VCF files.\n",
    "\n",
    "**Expected output:** Two VCF files (one for each sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c203a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"VARIANT CALLING WITH BCFTOOLS\")\n",
    "\n",
    "illumina_vcf = data_dir / \"illumina_variants.vcf.gz\"\n",
    "pacbio_vcf = data_dir / \"pacbio_variants.vcf.gz\"\n",
    "\n",
    "def call_variants(bam_path, vcf_path, sample_name):\n",
    "    \n",
    "    if vcf_path.exists():\n",
    "        print(f\"\\n{sample_name} VCF already exists: {vcf_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Calling variants for {sample_name}\")\n",
    "    print(f\"Running bcftools mpileup\")\n",
    "    mpileup_cmd = [\n",
    "        \"bcftools\", \"mpileup\",\n",
    "        \"-f\", str(chr10_fa_path),  # reference genome\n",
    "        \"-Ou\",                      # uncompressed BCF output\n",
    "        \"-q\", \"20\",                 # minimum mapping quality\n",
    "        \"-Q\", \"20\",                 # minimum base quality\n",
    "        str(bam_path)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running bcftools call\")\n",
    "    call_cmd = [\n",
    "        \"bcftools\", \"call\",\n",
    "        \"-mv\",                      # multiallelic and variants-only caller\n",
    "        \"-Ou\",                      # uncompressed BCF output\n",
    "        \"--ploidy\", \"2\"             # diploid organism\n",
    "    ]\n",
    "    \n",
    "    print(f\"Applying quality filters\")\n",
    "    filter_cmd = [\n",
    "        \"bcftools\", \"filter\",\n",
    "        \"-s\", \"LowQual\",            # mark low quality variants\n",
    "        \"-e\", \"QUAL<20\",            # filter expression: quality < 20\n",
    "        \"-Oz\",                      # compressed VCF output\n",
    "        \"-o\", str(vcf_path)\n",
    "    ]\n",
    "    \n",
    "    p1 = subprocess.Popen(mpileup_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    p2 = subprocess.Popen(call_cmd, stdin=p1.stdout, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    p1.stdout.close()  # Allow p1 to receive SIGPIPE if p2 exits\n",
    "    p3 = subprocess.Popen(filter_cmd, stdin=p2.stdout, stderr=subprocess.PIPE)\n",
    "    p2.stdout.close()\n",
    "    p3.wait()\n",
    "    \n",
    "    if p3.returncode != 0:\n",
    "        stderr = p3.stderr.read().decode()\n",
    "        raise subprocess.CalledProcessError(p3.returncode, filter_cmd, stderr=stderr)\n",
    "    \n",
    "    print(f\"Variants called successfully: {vcf_path}\")\n",
    "    \n",
    "    print(f\"Indexing VCF file\")\n",
    "    subprocess.run([\n",
    "        \"bcftools\", \"index\",\n",
    "        \"-t\",  # tabix index\n",
    "        str(vcf_path)\n",
    "    ], check=True)\n",
    "    print(f\"Index created: {vcf_path}.tbi\")\n",
    "    \n",
    "    print(f\"\\nVariant Statistics:\")\n",
    "    stats_result = subprocess.run([\n",
    "        \"bcftools\", \"stats\",\n",
    "        str(vcf_path)\n",
    "    ], capture_output=True, text=True, check=True)\n",
    "    \n",
    "    for line in stats_result.stdout.split('\\n'):\n",
    "        if line.startswith('SN'):\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) >= 4 and 'number of' in parts[2]:\n",
    "                print(f\"  {parts[2]}: {parts[3]}\")\n",
    "\n",
    "call_variants(illumina_sorted_bam, illumina_vcf, \"Illumina\")\n",
    "call_variants(pacbio_sorted_bam, pacbio_vcf, \"PacBio\")\n",
    "\n",
    "print(\"VARIANT CALLING COMPLETE\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  Illumina: {illumina_vcf}\")\n",
    "print(f\"  PacBio: {pacbio_vcf}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34fbed",
   "metadata": {},
   "source": [
    "## Step 4: \n",
    "\n",
    "Now phase the variant VCFs with HapCUT2. The output of these tools may be in HapCUT block format; if that happens, convert this file to the phased VCF format.\n",
    "\n",
    "Expected output: two VCF files (one for each sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PHASING VARIANTS WITH HAPCUT2\")\n",
    "\n",
    "# Define output paths for phasing\n",
    "illumina_fragments = data_dir / \"illumina_fragments.txt\"\n",
    "illumina_phased = data_dir / \"illumina_phased.txt\"\n",
    "illumina_phased_vcf = data_dir / \"illumina_phased.vcf\"\n",
    "\n",
    "pacbio_fragments = data_dir / \"pacbio_fragments.txt\"\n",
    "pacbio_phased = data_dir / \"pacbio_phased.txt\"\n",
    "pacbio_phased_vcf = data_dir / \"pacbio_phased.vcf\"\n",
    "\n",
    "def phase_variants(bam_path, vcf_path, fragments_path, phased_path, phased_vcf_path, sample_name):\n",
    "    \"\"\"Phase variants using HapCUT2\"\"\"\n",
    "    \n",
    "    if phased_vcf_path.exists():\n",
    "        print(f\"{sample_name} phased VCF already exists: {phased_vcf_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Phasing variants for {sample_name}\")    \n",
    "    vcf_for_phasing = vcf_path\n",
    "    temp_vcf = None\n",
    "    if str(vcf_path).endswith('.gz'):\n",
    "        print(f\"Decompressing VCF for extractHAIRS...\")\n",
    "        temp_vcf = data_dir / f\"{sample_name}_temp.vcf\"\n",
    "        subprocess.run([\n",
    "            \"bcftools\", \"view\",\n",
    "            str(vcf_path),\n",
    "            \"-o\", str(temp_vcf)\n",
    "        ], check=True)\n",
    "        vcf_for_phasing = temp_vcf\n",
    "        print(f\"Temporary VCF created: {temp_vcf}\")\n",
    "    \n",
    "    print(f\"Extracting haplotype-informative reads...\")\n",
    "    extracthairs_cmd = [\n",
    "        \"extractHAIRS\",\n",
    "        \"--bam\", str(bam_path),\n",
    "        \"--VCF\", str(vcf_for_phasing),\n",
    "        \"--out\", str(fragments_path),\n",
    "        \"--ref\", str(chr10_fa_path)\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(extracthairs_cmd, capture_output=True, text=True, check=True)\n",
    "    print(f\"Fragment file created: {fragments_path}\")\n",
    "    if result.stderr:\n",
    "        print(f\"  extractHAIRS info: {result.stderr[:200]}...\")\n",
    "    \n",
    "    print(f\"Running HapCUT2 phasing algorithm...\")\n",
    "    hapcut2_cmd = [\n",
    "        \"HAPCUT2\",\n",
    "        \"--fragments\", str(fragments_path),\n",
    "        \"--VCF\", str(vcf_for_phasing),\n",
    "        \"--output\", str(phased_path),\n",
    "        \"--outvcf\", \"1\"  # Output VCF directly from HAPCUT2\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(hapcut2_cmd, capture_output=True, text=True, check=True)\n",
    "    print(f\"✓ Phased haplotype blocks created: {phased_path}\")\n",
    "    \n",
    "    # HAPCUT2 with --outvcf 1 creates a .phased.VCF file (with capital VCF)\n",
    "    hapcut2_phased_vcf = data_dir / f\"{phased_path.name}.phased.VCF\"\n",
    "    \n",
    "    if hapcut2_phased_vcf.exists():\n",
    "        import shutil\n",
    "        shutil.move(str(hapcut2_phased_vcf), str(phased_vcf_path))\n",
    "        print(f\"Phased VCF created: {phased_vcf_path}\")\n",
    "    \n",
    "    if result.stdout:\n",
    "        print(f\"\\nPhasing Statistics:\")\n",
    "        for line in result.stdout.split('\\n')[:10]:  # Show first 10 lines\n",
    "            if line.strip():\n",
    "                print(f\"  {line}\")\n",
    "    \n",
    "    if temp_vcf and temp_vcf.exists():\n",
    "        temp_vcf.unlink()\n",
    "        print(f\"Cleaned up temporary VCF: {temp_vcf}\")\n",
    "    \n",
    "    print(f\"\\nPhasing Summary:\")\n",
    "    with open(phased_path, 'r') as f:\n",
    "        block_count = 0\n",
    "        variant_count = 0\n",
    "        for line in f:\n",
    "            if line.startswith('BLOCK'):\n",
    "                block_count += 1\n",
    "            elif not line.startswith('*') and line.strip():\n",
    "                variant_count += 1\n",
    "        print(f\"  Phased blocks: {block_count}\")\n",
    "        print(f\"  Phased variants: {variant_count}\")\n",
    "        \n",
    "phase_variants(illumina_sorted_bam, illumina_vcf, illumina_fragments, illumina_phased, illumina_phased_vcf, \"Illumina\")\n",
    "\n",
    "phase_variants( pacbio_sorted_bam, pacbio_vcf, pacbio_fragments, pacbio_phased, pacbio_phased_vcf, \"PacBio\")\n",
    "\n",
    "print(\"PHASING COMPLETE\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  Illumina phased VCF: {illumina_phased_vcf}\")\n",
    "print(f\"  PacBio phased VCF: {pacbio_phased_vcf}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9984d71",
   "metadata": {},
   "source": [
    "## Step 5:\n",
    "\n",
    "Now you should have two phased VCF files (one for each sequencing technology). Compare these VCFs. How many variants are shared between the VCFs? How many are not?\n",
    "\n",
    "Select 2-3 variants that are not common (if any) and check which technology supports this variant. Open both BAM files in IGV and take a screenshot of each problematic discordant location. What can you deduce from these screenshots—are these variants sequencing-related artifacts or are they indeed true variants?\n",
    "\n",
    "Do this analysis for every gene.\n",
    "\n",
    "IGV screenshots can also be automated (it is a bit tricky, though—ask your LLM for help). You can opt out of doing this, but you will lose half a point.\n",
    "\n",
    "Expected output: Jupyter cell(s) with IGV screenshots and a discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"COMPARING VCFS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTODO: Compare VCFs and identify discordant variants\")\n",
    "print(\"This step will analyze shared and unique variants between technologies\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c281267",
   "metadata": {},
   "source": [
    "## Step 6: Determine Star-Alleles using PharmVar\n",
    "\n",
    "Can you figure out the star-allele for each gene of interest? The star-allele database can be found in PharmVar; see this for CYP2C19. Your answer should be something like CYP2C19*12 because X, Y and Z. This step does not have to be automated, but should be at least explained in the notebook.\n",
    "\n",
    "Hint: use phased data!\n",
    "\n",
    "Expected output: Jupyter cell(s) with discussion (and code, if you want to do it that way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STAR-ALLELE DETERMINATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTODO: Determine star-alleles using PharmVar database\")\n",
    "print(\"This step will identify CYP2C8, CYP2C9, and CYP2C19 star-alleles\")\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
